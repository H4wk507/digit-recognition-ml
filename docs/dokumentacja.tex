\documentclass[12pt,a4paper]{article}
\usepackage[polish]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage[]{algorithm2e}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={black!50!black},
    citecolor={black!50!black},
    urlcolor={black!80!black}
}


\usepackage{color}
\usepackage{listings}

\lstloadlanguages{% Check Dokumentation for further languages ...
	C,
	C++,
	csh,
	Java
}

\definecolor{red}{rgb}{0.6,0,0} % for strings
\definecolor{blue}{rgb}{0,0,0.6}
\definecolor{green}{rgb}{0,0.8,0}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\lstset{
	language=csh,
	basicstyle=\footnotesize\ttfamily,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
	tabsize=2,
	extendedchars=true,
	breaklines=true,
	frame=b,
	stringstyle=\color{blue}\ttfamily,
	showspaces=false,
	showtabs=false,
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	commentstyle=\color{green},
	morecomment=[l]{//}, %use comment-line-style!
	morecomment=[s]{/*}{*/}, %for multiline comments
	showstringspaces=false,
	morekeywords={ abstract, event, new, struct,
		as, explicit, null, switch,
		base, extern, object, this,
		bool, false, operator, throw,
		break, finally, out, true,
		byte, fixed, override, try,
		case, float, params, typeof,
		catch, for, private, uint,
		char, foreach, protected, ulong,
		checked, goto, public, unchecked,
		class, if, readonly, unsafe,
		const, implicit, ref, ushort,
		continue, in, return, using,
		decimal, int, sbyte, virtual,
		default, interface, sealed, volatile,
		delegate, internal, short, void,
		do, is, sizeof, while,
		double, lock, stackalloc,
		else, long, static,
		enum, namespace, string},
	keywordstyle=\color{cyan},
	identifierstyle=\color{red},
}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{blue}{\parbox{\textwidth}{\hspace{15pt}#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize}}


\addtolength{\hoffset}{-1.5cm}
\addtolength{\marginparwidth}{-1.5cm}
\addtolength{\textwidth}{3cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2.5cm}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}

\begin{document}

\title{Systemy Sztucznej Inteligencji\\\small
	{Dokumentacja Projektu\\Porównanie algorytmu KNN oraz
		Naiwnego Klasyfikatora Bayesa przy klasyfikacji odręcznie pisanych cyfr.}}
\author{Piotr Skowroński gr. 3/6\\Krzysztof Czuba gr. 4/7\\Jakub Poreda gr. 3/6}
\date{\today}

\maketitle

\newpage
\tableofcontents

\newpage
\section{Wstęp}
\subsection{Opis programu}
Celem programu jest klasyfikowanie odręcznie pisanych cyfr przez użytkownika. Aplikacja zawiera proste GUI,
które pozwala użytkownikowi narysować cyfrę na płótnie, a następnie po wciśnięciu przycisku 'Rozpoznaj' program
klasyfikuje cyfrę za pomocą jednego z klasyfikatorów w celu rozpoznania narysowanej cyfry. Otrzymane wyniki
klasyfikacji są wyświetlane w bloku po prawej stronie interfejsu użytkownika.
\begin{figure}[!h]
	\includegraphics{"app1.png"}
	\centering
	\caption{Wygląd aplikacji}
\end{figure}
\newpage
\begin{figure}[!h]
	\includegraphics{"app2.png"}
	\centering
	\caption{Rozpoznawanie}
\end{figure}
\subsection{Użyte biblioteki}
Program korzysta z następujacych zewnętrznych bibliotek:\\
- Pillow\\
\indent - Do transformacji zapisanych cyfr na macierz\\
\indent - Do transformacji narysowanej cyfry na macierz\\
- numpy\\
- seaborn
\subsection{Baza danych}
Baza danych składa się z 1000 obrazów cyfr narysowanych przez nas (100 dla każdej cyfry). Każdy piksel obrazu jest reprezentowany w skali
szarości (ma wartość od 0 do 255, gdzie 0 to biały, a 255 to czarny kolor). Obrazy są przechowywane w formacie png.
\newpage
\begin{figure}[!h]
	\includegraphics[scale=0.8]{"number_example.png"}
	\centering
	\caption{Przykładowy obraz z bazy danych}
\end{figure}
\begin{figure}[!h]
	\includegraphics[scale=8]{"normalized.png"}
	\centering
	\caption{Ten sam obraz po zmianie rozdzielczości na 28x28 pikseli}
\end{figure}
\newpage
\section{Opis działania}
\subsection{Normalizacja danych}
Piksele wczytanego obrazu są konwertowane na skalę szarości tj. 0 - kolor biały, 255 - kolor czarny
oraz są normalizowane do przedziału od 0 do 1 co ułatwia modelowi dopasowanie cyfr.
Wzór na normalizację pojedyńczego piksela:
\[
	z_i=\frac{x_i - min(x)}{max(x) - min(x)}
\]
gdzie: \\
\indent $z_i$ - znormalizowany piksel\\
\indent $x_i$ - piksel\\
\indent $x$ - zbiór wszystkich pikseli\\~\\
Ostatecznie wzór ma postać:
\[
	z_i=\frac{x_i}{255}
\]
\subsection{Algorytm k najbliższych sąsiadów}
Klasyfikator kNN to jedna z ważniejszych nieparametrycznych metod klasyfikacji. W tej metodzie
klasyfikowany obiekt przydzielamy do tej klasy, do której należy większość z k sąsiadów.
\begin{figure}[!h]
	\includegraphics{"KnnClassification.png"}
	\centering
	\caption{Przykład klasyfikacji metodą kNN}
\end{figure}

W przypadku k=3 (mniejszy okrąg), zielona kropka zostanie zakwalifikowana do czerwonych
trójkątów. W przypadku k=5 (większy okrąg) - do niebieskich kwadratów.

\subsection{Metryka odległości}
Użyta została odległość Minkowskiego określona wzorem:
\[
	L_m(x, y) = \left(\sum_{i = 1}^{n} |x_i - y_i|^p\right)^\frac{1}{p}
\]
gdzie: \\
\indent $L_m$ - odległość między punktami x i y\\
\indent $x$, $y$ - punkty w przestrzeni $n$ wymiarowej\\
\indent $x_i$, $y_i$ - i'ta współrzędna punktów x i y\\
\indent $p$ - parametr określający rodzaj metryki\\

\noindent Przetestowaliśmy skuteczność klasyfikatora kNN dla liczby sąsiadów $k \in \{1, 3, 5, 7, 9, 11, 13, 15\}$ oraz
dla wartości parametru $p \in \{1, 2, 3\}$.

\subsection{Pseudokod algorytmu kNN}
\begin{algorithm}[H]
	\KwData{Dane wejściowe: zbiór treningowy $train\_data$
		zbiór testowy $test\_data$ liczba sąsiadów $k$ metryka $p$}
	\KwResult{Zbiór testowy z przewidzianymi etykietami}
	\ForEach{$test\_instance$ in $test\_data$}{
		$distances$ = $[]$\;
		\ForEach{$train\_instance$ in $train\_data$}{
			$distance$ = $point\_distance(test\_instance, train\_instance, p)$\;
			$distances.append((train\_instance, distance))$\;
		}
		$sorted\_distances = sort(distances, by=distance)$\;
		$k\_nearest\_neighbors = sorted\_distances[:k]$\;
		$test\_instance.set\_predicted\_label(predicted\_label)$\;
	}
	\Return $test\_data$\;
	\caption{Algorytm k najbliższych sąsiadów.}
\end{algorithm}

\subsection{Wyniki klasyfikatora kNN}
Po przeanalizowaniu różnych wartości parametrów $k$ i $p$, algorytm kNN klasyfikuje cyfry
z największą dokładnością na poziomie 94.9\% dla pary parametrów $k = 2$ i $p = 3$.
\newpage
\begin{figure}[!h]
	\includegraphics[scale=1.00]{"knn.png"}
	\centering
	\caption{Zależność dokładności klasyfikacji od liczby sąsiadów $k$ i parametru $p$}
\end{figure}
\newpage
\begin{figure}[!h]
	\includegraphics[scale=0.85]{"confusion_matrix_knn.png"}
	\centering
	\caption{Macierz błędu algorytmu kNN dla $k = 1$ i $p = 3$}
\end{figure}
\subsection{Algorytm Naiwnego Bayesa}
Klasyfikator Naiwnego Bayesa jest to klasyfikator probabilistyczny, oparty na twierdzeniu Bayesa.
Zakłada on niezależność cech. Naiwny klasyfikator bayesowski predyktuje klasę nowego obiektu w zbiorze na podstawie
prawdopodobieństwa warunkowego.
\[
	p(D_i | X_1, X_2, ..., X_n)
\]
gdzie:\\
\indent $D_i$ - klasa obiektu\\
\indent $X_1, X_2, ..., X_n$ - cechy obiektu\\~\\
Prawdopodobieństwo wystąpienia elementu w danym zbiorze
\[
	p(D_i) = \frac{|D_i|}{N}
\]
gdzie:\\
\indent $|D_i|$ - moc zbioru $D_i$\\
\indent $N$ - liczba wszystkich elementów w zbiorze
\subsection{Pseudokod algorytmu Naiwnego Bayesa}
\begin{algorithm}[H]
	\KwData{Dane wejściowe: Dane $D$ Nowy punkt $NP$}
	\KwResult{Przewidziana klasa dla NP}
	$probabilities$ = $[]$\;
	\ForEach{$c$ in $classes$}{
		$probabilities.append($probability of class c)\;
	}
	$chosen\_class$ = $None$\;
	$max\_probability$ = $-1$\;
	\ForEach{$p$, $c$ in $zip(probabilities, classes)$}{
		\If{$p > max\_probability$}{
			$max\_probability$ = $p$\;
			$chosen\_class$ = $c$\;
		}
	}
	\Return{$chosen\_class$}
	\caption{Algorytm Naiwnego Bayesa.}
\end{algorithm}

\subsection{Wyniki klasyfikatora Naiwnego Bayesa}
Po przeprowadzeniu kilkunasu testów algorytm Naiwnego Bayesa uzyskuje dokładność
na poziomie 88.7\%.
\newpage
\begin{figure}[!h]
	\includegraphics[scale=0.80]{"confusion_matrix_bayes.png"}
	\centering
	\caption{Macierz błędu algorytmu Naiwnego Bayesa}
\end{figure}

\subsection{Implementacja}
\begin{enumerate}
	\item main.py - główny plik programu
	\item knn.py - plik zawierający implementację algorytmu kNN
	\item bayes.py - plik zawierający implementację naiwnego klasyfikatora Bayesa
	\item utils.py - plik zawierający funkcje pomocnicze (np. wczytywanie danych)
	\item test.py - plik zawierający funkcje testujące klasyfikatory
\end{enumerate}
\begin{lstlisting}
# Algorytm kNN
class KNN:
    def __init__(self, k: int = 3, p: int = 2):
        self.k = k
        self.p = p

    @staticmethod
    def minkowski_distance(X1, X2, p) -> float:
        return sum([abs((a - b) ** p) for a, b in zip(X1, X2)]) ** (1 / p)

    def fit(self, X, Y) -> None:
        self.X_train = X
        self.Y_train = Y

    def predict(self, X) -> list[float]:
        return [self.predict_point(x) for x in X]

    def predict_point(self, x) -> float:
        distances = [
            KNN.minkowski_distance(x, x_train, self.p)
            for x_train in self.X_train
        ]
        k_indices = np.argsort(distances)[: self.k]
        k_nearest_labels = [self.Y_train[i] for i in k_indices]
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

# Algorytm Naiwnego Bayesa
class NaiveBayesClassifier:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.num_classes = len(self.classes)
        self.num_features = X.shape[1]
        self.class_probs = np.zeros(self.num_classes)
        self.feature_probs = np.zeros((self.num_classes, self.num_features))
        for i, c in enumerate(self.classes):
            X_c = X[y == c]
            self.class_probs[i] = len(X_c) / len(X)
            self.feature_probs[i] = np.mean(X_c, axis=0)

    def predict(self, X):
        y_pred = np.zeros(X.shape[0], dtype=np.str_)
        for i, x in enumerate(X):
            posterior_probs = []
            for j in range(self.num_classes):
                prior = np.log(self.class_probs[j])
                likelihood = np.sum(
                    np.log(self.compute_feature_prob(self.feature_probs[j], x))
                )
                posterior = prior + likelihood
                posterior_probs.append(posterior)
            y_pred[i] = self.classes[np.argmax(posterior_probs)]
        return y_pred

    def compute_feature_prob(self, feature_prob, x):
        epsilon = 1e-9
        return feature_prob * x + (1 - feature_prob) * (1 - x + epsilon)

# Funkcja klasyfikujaca narysowana cyfre
def predict_digit(img):
    img = img.resize((28, 28))
    img = img.convert("L")
    img = np.array(img).flatten()
    img = np.invert(img)
    img = img / 255
    x_train, y_train = read_digits("imgs")
    x_train = x_train / 255
    model = KNeighborsClassifier(n_neighbors=1, p=3)
    model.fit(x_train, y_train)
    y_pred = model.predict(np.array([img]))[0]
    return y_pred
\end{lstlisting}
\section{Wnioski}
\subsection{Porównanie wyników klasyfikatorów}
W badaniu porównującym algorytm kNN i naiwny klasyfikator
bayesowski w rozpoznawaniu pisma odręcznego stwierdzono, że k-NN osiągnął lepsze
wyniki i dokładność klasyfikacji niż naiwny klasyfikator bayesowski. Wykorzystano
zbiór danych, który został podzielony na dane treningowe (75\%) i dane testowe (25\%),
gdzie każda próbka była reprezentowana jako obraz 28 x 28 pikseli przedstawiający odręczne cyfry.
Naiwny klasyfikator bayesowski uzyskał dokładność na poziomie około 88.7\%, podczas gdy kNN osiągnął
dokładność na poziomie około 94.9\%. Wynika z tego, że k-NN
jest bardziej skutecznym narzędziem do rozpoznawania cyfr pisma odręcznego niż naiwny klasyfikator
bayesowski, zwłaszcza w przypadku większej liczby danych treningowych i większego wymiaru wejściowego.
Naiwny klasyfikator bayesowski ma jednak pewne zalety, takie jak krótszy czas potrzebny na dopasowanie i
testowanie. W porównaniu do tego, klasyfikator kNN wymagał znacznie więcej czasu. Warto jednak zauważyć,
że k-NN może być dobrym wyborem dla mniejszych zbiorów danych i problemów o mniejszej liczbie wymiarów.
\subsection{Potencjał rozwoju}

Projekt rozpoznawania cyfr przy użyciu uczenia maszynowego ma potencjał rozwoju w wielu różnych obszarach:
- Zwiększenie dokładności rozpoznawania: Można podjąć działania mające na celu zwiększenie dokładności
rozpoznawania cyfr. Można eksperymentować z różnymi algorytmami uczenia maszynowego, takimi
jak głębokie sieci neuronowe, konwolucyjne sieci neuronowe (CNN).

- Rozpoznawanie innych zestawów danych: Oprócz rozpoznawania cyfr można rozważyć rozwinięcie projektu w celu
rozpoznawania innych zestawów danych. Można badać możliwość rozpoznawania liter, symboli matematycznych
lub innych obiektów.

- Adaptacja do innych języków i pism: Projekty rozpoznawania cyfr można rozszerzyć na inne języki i pisma.
Na przykład, można badać możliwość rozpoznawania cyfr w języku chińskim, arabskim, japońskim lub innych językach,
które posiadają swoje własne znaki i pismo. Odpowiednie dane treningowe i dostosowanie modelu
będą kluczowe w takim przypadku.
\section{Pełen kod aplikacji}
\subsection{main.py}
\begin{lstlisting}
import os
import sys
import tkinter as tk
from datetime import datetime

import numpy as np
from PIL import Image, ImageGrab  # Pillow
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier  # scikit-learn

DIRNAME = os.path.dirname(os.path.dirname(__file__))
sys.path.append(DIRNAME)

from digit_recognition.bayes import NaiveBayesClassifier
from digit_recognition.knn import KNN
from digit_recognition.utils import read_digits


class Settings:
    WIDTH = HEIGHT = 300
    FONTSIZE = 20


def predict_digit(img):
    img = img.resize((28, 28))
    img = img.convert("L")
    img = np.array(img).flatten()
    img = np.invert(img)
    img = img / 255
    x_train, y_train = read_digits("imgs")
    x_train = x_train / 255
    model = KNeighborsClassifier(n_neighbors=1, p=3)
    model.fit(x_train, y_train)
    y_pred = model.predict(np.array([img]))[0]
    return y_pred


class App(tk.Tk):
    def __init__(self):
        tk.Tk.__init__(self)
        tk.Tk.title(self, "Rozpoznawanie cyfr")
        self.x = self.y = 0
        self.canvas = tk.Canvas(
            self,
            width=Settings.WIDTH,
            height=Settings.HEIGHT,
            bg="white",
            cursor="cross",
        )
        self.label = tk.Label(
            self, text="?", font=("Helvetica", Settings.FONTSIZE)
        )
        self.classify_button = tk.Button(
            self, text="Rozpoznaj", command=self.classify_handwriting
        )
        self.clear_button = tk.Button(
            self, text="Wyczysc", command=self.clear_all
        )
        self.save_button = tk.Button(
            self, text="Zapisz", command=self.save_to_file
        )
        self.canvas.grid(
            row=0,
            column=0,
            pady=2,
            sticky=tk.W,
        )
        self.label.grid(row=0, column=1, pady=2, padx=2)
        self.classify_button.grid(row=1, column=1, pady=2, padx=2)
        self.clear_button.grid(row=1, column=0, pady=2)
        self.save_button.grid(row=1, column=2, pady=2)
        self.canvas.bind("<B1-Motion>", self.draw_lines)

    def clear_all(self):
        self.canvas.delete("all")

    def get_canvas_image(self) -> Image:
        x = self.winfo_rootx() + self.canvas.winfo_x()
        y = self.winfo_rooty() + self.canvas.winfo_y()
        x1 = x + self.canvas.winfo_width()
        y1 = y + self.canvas.winfo_height()
        return ImageGrab.grab().crop((x, y, x1, y1))

    def classify_handwriting(self):
        im = self.get_canvas_image()
        digit = predict_digit(im)
        self.label.configure(text=str(digit))

    def save_to_file(self):
        folder = "imgs"
        filename = datetime.today().strftime("%d-%m-%Y %Hh%Mm%Ss") + ".png"
        im = self.get_canvas_image()
        im.save(os.path.join(folder, filename))

    def draw_lines(self, event):
        self.x = event.x
        self.y = event.y
        r = 20
        self.canvas.create_oval(
            self.x - r,
            self.y - r,
            self.x + r,
            self.y + r,
            fill="black",
            outline="black",
        )


def main() -> int:
    app = App()
    app.mainloop()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

\end{lstlisting}
\subsection{bayes.py}
\begin{lstlisting}
	import numpy as np


	class NaiveBayesClassifier:
		def fit(self, X, y):
			self.classes = np.unique(y)
			self.num_classes = len(self.classes)
			self.num_features = X.shape[1]
			self.class_probs = np.zeros(self.num_classes)
			self.feature_probs = np.zeros((self.num_classes, self.num_features))
			for i, c in enumerate(self.classes):
				X_c = X[y == c]
				self.class_probs[i] = len(X_c) / len(X)
				self.feature_probs[i] = np.mean(X_c, axis=0)
	
		def predict(self, X):
			y_pred = np.zeros(X.shape[0], dtype=np.str_)
			for i, x in enumerate(X):
				posterior_probs = []
				for j in range(self.num_classes):
					prior = np.log(self.class_probs[j])
					likelihood = np.sum(
						np.log(self.compute_feature_prob(self.feature_probs[j], x))
					)
					posterior = prior + likelihood
					posterior_probs.append(posterior)
				y_pred[i] = self.classes[np.argmax(posterior_probs)]
			return y_pred
	
		def compute_feature_prob(self, feature_prob, x):
			epsilon = 1e-9
			return feature_prob * x + (1 - feature_prob) * (1 - x + epsilon)
	
		def accuracy_score(self, y_test, y_pred):
			return sum([y_test[i] == y_pred[i] for i in range(len(y_test))]) / len(
				y_test
			)
\end{lstlisting}
\subsection{knn.py}
\begin{lstlisting}
	from collections import Counter

	import numpy as np
	
	
	class KNN:
		def __init__(self, k: int = 3, p: int = 2):
			self.k = k
			self.p = p
	
		@staticmethod
		def minkowski_distance(X1, X2, p) -> float:
			return sum([abs((a - b) ** p) for a, b in zip(X1, X2)]) ** (1 / p)
	
		def fit(self, X, Y) -> None:
			self.X_train = X
			self.Y_train = Y
	
		def predict(self, X) -> list[float]:
			return [self.predict_point(x) for x in X]
	
		def predict_point(self, x) -> float:
			distances = [
				KNN.minkowski_distance(x, x_train, self.p)
				for x_train in self.X_train
			]
			k_indices = np.argsort(distances)[: self.k]
			k_nearest_labels = [self.Y_train[i] for i in k_indices]
			most_common = Counter(k_nearest_labels).most_common(1)
			return most_common[0][0]
	
		def accuracy_score(self, y_test, y_pred):
			return sum([y_test[i] == y_pred[i] for i in range(len(y_test))]) / len(
				y_test
			)
\end{lstlisting}
\subsection{test.py}
\begin{lstlisting}
	import os
	import sys
	from collections import defaultdict
	
	import matplotlib.pyplot as plt
	from sklearn.metrics import accuracy_score, confusion_matrix
	from sklearn.model_selection import train_test_split
	from sklearn.neighbors import KNeighborsClassifier
	
	DIRNAME = os.path.dirname(os.path.dirname(__file__))
	sys.path.append(DIRNAME)
	
	
	from digit_recognition.bayes import NaiveBayesClassifier
	from digit_recognition.knn import KNN
	from digit_recognition.utils import read_digits
	
	
	def test_knn(
		X,
		y,
		k_list: list[int],
		p_list: list[int],
		filename: str,
		n_tests: int = 100,
		plot: bool = False,
		colors: list[str] = [],
	) -> None:
		m2 = {}
		for p in p_list:
			m = defaultdict(float)
			for _ in range(n_tests):
				for k in k_list:
					X_train, X_test, y_train, y_test = train_test_split(
						X, y, train_size=0.75
					)
					model = KNeighborsClassifier(n_neighbors=k, p=p)
					model.fit(X_train, y_train)
					y_pred = model.predict(X_test)
					accuracy = accuracy_score(y_test, y_pred)
					m[k] += 100 * accuracy / n_tests
			m2[p] = m
		if plot:
			assert len(colors) == len(p_list)
			for i, (k, v) in enumerate(m2.items()):
				plt.scatter(k_list, v.values(), label=f"p={k}", color=colors[i])
			plt.grid()
			plt.xlabel("k neighbors")
			plt.ylabel("Accuracy [%]")
			plt.legend([f"p = {p}" for p in p_list], loc="upper right")
			plt.xticks(k_list)
			plt.savefig(filename)
		else:
			print(m2)
	
	
	def test_bayes(X, y, n_tests: int = 100) -> None:
		acc = 0
		for _ in range(n_tests):
			X_train, X_test, y_train, y_test = train_test_split(
				X, y, train_size=0.75
			)
			model = NaiveBayesClassifier()
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			accuracy = model.accuracy_score(y_test, y_pred)
			acc += 100 * accuracy / n_tests
		print(f"accuracy: {round(acc, 3)}")
\end{lstlisting}
\subsection{utils.py}
\begin{lstlisting}
	import os

	import numpy as np
	from PIL import Image
	
	
	def read_digits(dirname: str):
		"""Read digits' images from dirname directory.
		Return a tuple of two lists:
		- X: list of image pixels (each pixel is a number from
		0 (white) to 255 (black).
		- y: list of digit labels from 0 to 9.
		Dimensions:
		- X: nsamples * 28 * 28 (each image is resized to 28x28 pixels).
		- y: nsamples."""
		X = []
		y = []
		for subdir, _, files in os.walk(dirname):
			for file in files:
				label = subdir[-1]
				filepath = subdir + os.sep + file
				img = Image.open(filepath)
				img = img.resize((28, 28))
				img = img.convert("L")
				img = np.array(img).flatten()
				img = np.invert(img)
				X.append(img)
				y.append(label)
		return np.array(X), np.array(y)
\end{lstlisting}
\end{document}
