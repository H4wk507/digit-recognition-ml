\documentclass[12pt,a4paper]{article}
\usepackage[polish]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage[]{algorithm2e}
\usepackage{listings}
\usepackage{graphicx}


\usepackage{color}
\usepackage{listings}

\lstloadlanguages{% Check Dokumentation for further languages ...
	C,
	C++,
	csh,
	Java
}

\definecolor{red}{rgb}{0.6,0,0} % for strings
\definecolor{blue}{rgb}{0,0,0.6}
\definecolor{green}{rgb}{0,0.8,0}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\lstset{
	language=csh,
	basicstyle=\footnotesize\ttfamily,
	numbers=left,
	numberstyle=\tiny,
	numbersep=5pt,
	tabsize=2,
	extendedchars=true,
	breaklines=true,
	frame=b,
	stringstyle=\color{blue}\ttfamily,
	showspaces=false,
	showtabs=false,
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	commentstyle=\color{green},
	morecomment=[l]{//}, %use comment-line-style!
	morecomment=[s]{/*}{*/}, %for multiline comments
	showstringspaces=false,
	morekeywords={ abstract, event, new, struct,
		as, explicit, null, switch,
		base, extern, object, this,
		bool, false, operator, throw,
		break, finally, out, true,
		byte, fixed, override, try,
		case, float, params, typeof,
		catch, for, private, uint,
		char, foreach, protected, ulong,
		checked, goto, public, unchecked,
		class, if, readonly, unsafe,
		const, implicit, ref, ushort,
		continue, in, return, using,
		decimal, int, sbyte, virtual,
		default, interface, sealed, volatile,
		delegate, internal, short, void,
		do, is, sizeof, while,
		double, lock, stackalloc,
		else, long, static,
		enum, namespace, string},
	keywordstyle=\color{cyan},
	identifierstyle=\color{red},
}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{blue}{\parbox{\textwidth}{\hspace{15pt}#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize}}


\addtolength{\hoffset}{-1.5cm}
\addtolength{\marginparwidth}{-1.5cm}
\addtolength{\textwidth}{3cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2.5cm}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}

% TODO: wjebać cały ten plik na jakiś ortograf.pl
\begin{document}

\title{Systemy Sztucznej Inteligencji\\\small
	{Dokumentacja Projektu\\Porównanie algorytmu KNN oraz
		Naiwnego Klasyfikatora Bayesa przy klasyfikacji odręcznie pisanych cyfr.}}
\author{Piotr Skowroński gr. 3/6\\Krzysztof Czuba gr. 4/7\\Jakub Poreda gr. 3/6}
\date{\today}

\maketitle
\newpage
\section{Wstęp}
\subsection{Opis programu}
Celem programu jest klasyfikowanie odręcznie pisanych cyfr przez użytkownika. Aplikacja zawiera proste GUI,
które pozwala użytkownikowi narysować cyfrę na płótnie, a następnie po wciśnięciu przycisku 'Rozpoznaj' program
klasyfikuje cyfrę za pomocą jednego z klasyfikatorów w celu rozpoznania narysowanej cyfry. Otrzymane wyniki
klasyfikacji są wyświetlane w bloku po prawej stronie interfejsu użytkownika.
\begin{figure}[!h]
	\includegraphics{"app1.png"}
	\centering
	\caption{Wygląd aplikacji}
\end{figure}
\newpage
\begin{figure}[!h]
	\includegraphics{"app2.png"}
	\centering
	\caption{Rozpoznawanie}
\end{figure}
\subsection{Użyte biblioteki}
Program korzysta z następujacych zewnętrznych bibliotek:\\
- Pillow\\
\indent - Do transformacji zapisanych cyfr na macierz\\
\indent - Do transformacji narysowanej cyfry na macierz\\
- numpy\\
- seaborn
\subsection{Baza danych}
Baza danych składa się z 1000 obrazów cyfr narysowanych przez nas (100 dla każdej cyfry). Każdy piksel obrazu jest reprezentowany w skali
szarości (ma wartość od 0 do 255, gdzie 0 to biały, a 255 to czarny kolor). Obrazy są przechowywane w formacie png.
\newpage
\begin{figure}[!h]
	\includegraphics[scale=0.8]{"number_example.png"}
	\centering
	\caption{Przykładowy obraz z bazy danych}
\end{figure}
\begin{figure}[!h]
	\includegraphics[scale=8]{"normalized.png"}
	\centering
	\caption{Ten sam obraz po zmianie rozdzielczości na 28x28 pikseli}
\end{figure}
\newpage
\section{Opis działania}
\subsection{Normalizacja danych}
% Tutaj uwzględniamy część matematyczną. Opisujemy całą teorię np.:
% dla zadania związanego z sieciami neuronowymi - opisujemy całą budowę, algorytm uczenia i wszystkie wzory. 
% mile widziany przykład obliczeniowy).
Piksele wczytanego obrazu są konwertowane na skalę szarości tj. 0 - kolor biały, 255 - kolor czarny
oraz są normalizowane do przedziału od 0 do 1 co ułatwia modelowi dopasowanie cyfr.
Wzór na normalizację pojedyńczego piksela:
\[
	z_i=\frac{x_i - min(x)}{max(x) - min(x)}
\]
gdzie: \\
\indent $z_i$ - znormalizowany piksel\\
\indent $x_i$ - piksel\\
\indent $x$ - zbiór wszystkich pikseli\\~\\
Ostatecznie wzór ma postać:
\[
	z_i=\frac{x_i}{255}
\]
\subsection{Algorytm k najbliższych sąsiadów}
Klasyfikator kNN to jedna z ważniejszych nieparametrycznych metod klasyfikacji. W tej metodzie
klasyfikowany obiekt przydzielamy do tej klasy, do której należy większość z k sąsiadów.
\begin{figure}[!h]
	\includegraphics{"KnnClassification.png"}
	\centering
	\caption{Przykład klasyfikacji metodą kNN}
\end{figure}

W przypadku k=3 (mniejszy okrąg), zielona kropka zostanie zakwalifikowana do czerwonych
trójkątów. W przypadku k=5 (większy okrąg) - do niebieskich kwadratów.

%Następnie przechodzimy do fazy uczenia, gdzie model otrzymuje nasze dane treningowe.
%Model wykorzystując klasyfikator KNN, umieszcza w przestrzeni metrycznej wartości pikseli cyfr
%z zestawu treningowego. Wykorzstuje metrykę odległości Manhattan mierzy on sumę bezwzględnych różnic pomiędzy cechami
%\[
%	d_{n}=\Sigma^{n}_{i=0}|wektor1_{i}-wektor2_{i}|
%\]
%Przewidywanie odbywa się poprzez wybranie trzech cyfr z zbioru treningowego, których cechy różnią się najmniej od cech sprawdzanego rysunku. Wybór odbywa się poprzez znalezienie dominującej spośród wybranych cyfr

\subsection{Metryka odległości}
Użyta została odległość Minkowskiego określona wzorem:
\[
	L_m(x, y) = \left(\sum_{i = 1}^{n} |x_i - y_i|^p\right)^\frac{1}{p}
\]
gdzie: \\
\indent $L_m$ - odległość między punktami x i y\\
\indent $x$, $y$ - punkty w przestrzeni $n$ wymiarowej\\
\indent $x_i$, $y_i$ - i'ta współrzędna punktów x i y\\
\indent $p$ - parametr określający rodzaj metryki\\

\noindent Przetestowaliśmy skuteczność klasyfikatora kNN dla liczby sąsiadów $k \in \{1, 3, 5, 7, 9, 11, 13, 15\}$ oraz
dla wartości parametru $p \in \{1, 2, 3\}$.

\subsection{Pseudokod algorytmu kNN}
% Tutaj opisujemy rozwiązanie zadania. Dla przedmiotu programowanie 
% będzie to wykorzystanie matematyki z poprzedniego zadania itd. 
% Dla SSI będzie to ogólne działanie przetwarzania danych w oparciu o 
% modele matematyczne z poprzedniego zadania.
\begin{algorithm}[H]
	\KwData{Dane wejściowe: zbiór treningowy $train\_data$
		zbiór testowy $test\_data$ liczba sąsiadów $k$ metryka $p$}
	\KwResult{Zbiór testowy z przewidzianymi etykietami}
	\ForEach{$test\_instance$ in $test\_data$}{
		$distances$ = $[]$\;
		\ForEach{$train\_instance$ in $train\_data$}{
			$distance$ = $point\_distance(test\_instance, train\_instance, p)$\;
			$distances.append((train\_instance, distance))$\;
		}
		$sorted\_distances = sort(distances, by=distance)$\;
		$k\_nearest\_neighbors = sorted\_distances[:k]$\;
		$test\_instance.set\_predicted\_label(predicted\_label)$\;
	}
	\Return $test\_data$\;
	\caption{Algorytm k najbliższych sąsiadów.}
\end{algorithm}

\subsection{Implementacja}
% Opis, zasada i działanie programu ze względu na podział na pliki, nastepnie 
% funkcje programu wraz ze szczegółowym opisem działania (np.: formie pseudokodu, czy odniesienia do równania)
\begin{enumerate}
	\item main.py - główny plik programu
	\item knn.py - plik zawierający implementację algorytmu kNN
	\item bayes.py - plik zawierający implementację naiwnego klasyfikatora Bayesa
	\item utils.py - plik zawierający funkcje pomocnicze (np. wczytywanie danych)
	\item test.py - plik zawierający funkcje testujące klasyfikatory
\end{enumerate}
% TODO: dodać kod do funkcji: predict, predict_point, predict_digit
\begin{lstlisting}
	print("Hello world!")
\end{lstlisting}
\subsection{Testy}
Tutaj powinna pojawić się analiza uzyskanych wyników oraz wykresy/pomiary.
\subsection{Eksperymenty}
\begin{figure}[!h]
	\includegraphics[scale=1]{"knn.png"}
	\centering
	\caption{Zależność dokładności klasyfikacji od liczby sąsiadów $k$ i parametru $p$}
\end{figure}
Najlepsze wyniki klasyfikator kNN uzyskuje dla $k = 1$ oraz $p = 3$, gdzie jego dokładność wynosi 94.9\%.
% Sekcję używamy gdy porównywaliśmy dwa lub więcej algorytmów, albo wykonywaliśmy jakies pomiary.
% Warto dodać jakies wykresy jako obraz, albo tabele z wynikami. 
% Wszyskie wyniki powinny być opisane/poddane komentarzowi i poddane analizie statystycznej.
\newpage
\section{Pełen kod aplikacji}
\begin{lstlisting}
Tutaj wklejamy pelen kod. 
\end{lstlisting}
\end{document}
